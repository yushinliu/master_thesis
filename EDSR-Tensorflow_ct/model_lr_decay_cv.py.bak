import tensorflow.contrib.slim as slim
import scipy.misc
import tensorflow as tf
#from tqdm import tqdm
from process_bar import ShowProcess
import numpy as np
import shutil
import utils
import os
import time 

"""
An implementation of the neural network used for
super-resolution of images as described in:

`Enhanced Deep Residual Networks for Single Image Super-Resolution`
(https://arxiv.org/pdf/1707.02921.pdf)

(single scale baseline-style model)
"""
class EDSR(object):

	def __init__(self,img_size=32,num_layers=32,feature_size=256,scale=2,output_channels=1):
		tf.reset_default_graph()
		print("Building EDSR...")
		for i in range(5):
			with tf.device('/gpu:%d' %i ):
				with tf.name_scope('crossvalidation%d' %i):
					#print("Building EDSR...")
					self.img_size = img_size
					self.scale = scale
					self.output_channels = output_channels

					#Placeholder for image inputs
					self.input = x = tf.placeholder(tf.float32,[None,img_size,img_size,output_channels],name='x')
					#Placeholder for upscaled image ground-truth
					self.target = y = tf.placeholder(tf.float32,[None,img_size*scale,img_size*scale,output_channels],name='y')
				
					"""
					Preprocessing as mentioned in the paper, by subtracting the mean
					However, the subtract the mean of the entire dataset they use. As of
					now, I am subtracting the mean of each batch
					"""
					mean_x =tf.reduce_mean(self.input) #127
					image_input =x- mean_x
					mean_y =tf.reduce_mean(self.target) #127
					image_target =y- mean_y 

					#One convolution before res blocks and to convert to required feature depth
					x = slim.conv2d(image_input,feature_size,[3,3])
				
					#Store the output of the first convolution to add later
					conv_1 = x	

					"""
					This creates `num_layers` number of resBlocks
					a resBlock is defined in the paper as
					(excuse the ugly ASCII graph)
					x
					|\
					| \
					|  conv2d
					|  relu
					|  conv2d
					| /
					|/
					+ (addition here)
					|
					result
					"""

					"""
					Doing scaling here as mentioned in the paper:

					`we found that increasing the number of feature
					maps above a certain level would make the training procedure
					numerically unstable. A similar phenomenon was
					reported by Szegedy et al. We resolve this issue by
					adopting the residual scaling with factor 0.1. In each
					residual block, constant scaling layers are placed after the
					last convolution layers. These modules stabilize the training
					procedure greatly when using a large number of filters.
					In the test phase, this layer can be integrated into the previous
					convolution layer for the computational efficiency.'

					"""
					scaling_factor = 0.1
					
					#Add the residual blocks to the model
					for i in range(num_layers):
						x = utils.resBlock(x,feature_size,scale=scaling_factor)

					#One more convolution, and then we add the output of our first conv layer
					x = slim.conv2d(x,feature_size,[3,3])
					x += conv_1
					
					#Upsample output of the convolution		
					x = utils.upsample(x,scale,feature_size,None)

					#One final convolution on the upsampling output
					output = x #utils.last_resBlock(x,output_channels,feature_size,scale=scaling_factor)
					#self.out_noconv = tf.clip_by_value(x+mean_x,0.0,255.0) # compress and see the image before last conv
					self.out = tf.clip_by_value(output+mean_x,0.0,255.0)  # compress all the output+mean_x into 0.0,255.0

					self.loss = loss = tf.reduce_mean(tf.losses.absolute_difference(image_target,output))
				
					#Calculating Peak Signal-to-noise-ratio
					#Using equations from here: https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio
					mse = tf.reduce_mean(tf.squared_difference(image_target,output))	
					PSNR = tf.constant(255**2,dtype=tf.float32)/mse
					PSNR = tf.constant(10,dtype=tf.float32)*utils.log10(PSNR)
				
					#Scalar to keep track for loss
					tf.summary.scalar("loss",self.loss)
					tf.summary.scalar("PSNR",PSNR)
					#Image summaries for input, target, and output
					tf.summary.image("input_image",tf.cast(self.input,tf.uint8)) #tf.cast: transfer the type of data
					tf.summary.image("target_image",tf.cast(self.target,tf.uint8))
					tf.summary.image("output_image",tf.cast(self.out,tf.uint8))
					#tf.summary.image("output_image_noconv",tf.cast(self.out_noconv,tf.uint8))
					
					#Tensorflow graph setup... session, saver, etc.
					#config = tf.ConfigProto()
					#config.gpu_options.per_process_gpu_memory_fraction = 0.8
					config=tf.ConfigProto(allow_soft_placement=True)
					self.sess = tf.Session(config=config)#config = config)#config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) # allow_soft_placement : allow to change the instrument if not exist
					self.saver = tf.train.Saver()
		print("Done building!")
	
	"""
	Save the current state of the network to file
	"""
	def save(self,savedir='saved_models'):
		print("Saving...")
		self.saver.save(self.sess,savedir+"/model")
		print("Saved!")
		
	"""
	Resume network from previously saved weights
	"""
	def resume(self,savedir='saved_models'):
		print("Restoring...")
		self.saver.restore(self.sess,tf.train.latest_checkpoint(savedir))
		print("Restored!")	

	"""
	Compute the output of this network given a specific input

	x: either one of these things:
		1. A numpy array of shape [image_width,image_height,3]
		2. A numpy array of shape [n,input_size,input_size,3]

	return: 	For the first case, we go over the entire image and run super-resolution over windows of the image
			that are of size [input_size,input_size,3]. We then stitch the output of these back together into the
			new super-resolution image and return that

	return  	For the second case, we return a numpy array of shape [n,input_size*scale,input_size*scale,3]
	"""
	def predict(self,x):
		print("Predicting...")
		if (len(x.shape) == 3) and not(x.shape[0] == self.img_size and x.shape[1] == self.img_size):
			num_across = x.shape[0]//self.img_size
			num_down = x.shape[1]//self.img_size
			tmp_image = np.zeros([x.shape[0]*self.scale,x.shape[1]*self.scale,3])
			for i in range(num_across):
				for j in range(num_down):
					tmp = self.sess.run(self.out,feed_dict={self.input:[x[i*self.img_size:(i+1)*self.img_size,j*self.img_size:(j+1)*self.img_size]]})[0]
					tmp_image[i*tmp.shape[0]:(i+1)*tmp.shape[0],j*tmp.shape[1]:(j+1)*tmp.shape[1]] = tmp
			#this added section fixes bottom right corner when testing
			if (x.shape[0]%self.img_size != 0 and  x.shape[1]%self.img_size != 0):
				tmp = self.sess.run(self.out,feed_dict={self.input:[x[-1*self.img_size:,-1*self.img_size:]]})[0]
				tmp_image[-1*tmp.shape[0]:,-1*tmp.shape[1]:] = tmp
					
			if x.shape[0]%self.img_size != 0:
				for j in range(num_down):
					tmp = self.sess.run(self.out,feed_dict={self.input:[x[-1*self.img_size:,j*self.img_size:(j+1)*self.img_size]]})[0]
					tmp_image[-1*tmp.shape[0]:,j*tmp.shape[1]:(j+1)*tmp.shape[1]] = tmp
			if x.shape[1]%self.img_size != 0:
				for j in range(num_across):
                                        tmp = self.sess.run(self.out,feed_dict={self.input:[x[j*self.img_size:(j+1)*self.img_size,-1*self.img_size:]]})[0]
                                        tmp_image[j*tmp.shape[0]:(j+1)*tmp.shape[0],-1*tmp.shape[1]:] = tmp
			return tmp_image
		else:
			return self.sess.run(self.out,feed_dict={self.input:x})

	"""
	Function to setup your input data pipeline
	"""
	def set_data_fn(self,fn,args,test_set_fn=None,test_set_args=None):
		self.data = fn
		self.args = args
		self.test_data = test_set_fn
		self.test_args = test_set_args

	"""
	Train the neural network
	"""

	def train(self,decay_steps=1000,decay_rate=0.9,iterations=4000,save_dir="saved_models"):
		#Removing previous save directory if there is one
		if os.path.exists(save_dir):
			shutil.rmtree(save_dir)
		#Make new save directory
		os.mkdir(save_dir)
		#Just a tf thing, to merge all summaries into one
		merged = tf.summary.merge_all()

		#add the decayed learning rate 
		initial_learning_rate=0.001
		learning_rate=tf.train.exponential_decay(initial_learning_rate,\
		global_step=iterations,decay_steps=decay_steps,decay_rate=decay_rate)


		#Using adam optimizer as mentioned in the paper
		optimizer = tf.train.AdamOptimizer(learning_rate)
		#This is the train operation for our objective
		train_op = optimizer.minimize(self.loss)	
		#Operation to initialize all variables
		init = tf.global_variables_initializer()
		print("Begin training...")
		with self.sess as sess:
			

			#Initialize all variables
			sess.run(init)
			test_exists = self.test_data
			#create summary writer for train
			train_writer = tf.summary.FileWriter(save_dir+"/train"+"_lr_iter_"+str(iterations)+"_decaysteps_"+str(decay_steps)+"_decay_rate_"+str(decay_rate),sess.graph)

			#If we're using a test set, include another summary writer for that
			
			if test_exists:
				test_writer = tf.summary.FileWriter(save_dir+"/test"+"_lr_iter_"+str(iterations)+"_decaysteps_"+str(decay_steps)+"_decay_rate_"+str(decay_rate),sess.graph)
				test_x,test_y = self.test_data(self.test_args)
				test_feed = {					
					'crossvalidation0/x':test_x[0],
					'crossvalidation1/x':test_x[1],
					'crossvalidation2/x':test_x[2],
					'crossvalidation3/x':test_x[3],
					'crossvalidation4/x':test_x[4],
					'crossvalidation0/y':test_y[0],
					'crossvalidation0/y':test_y[1],
					'crossvalidation0/y':test_y[2],
					'crossvalidation0/y':test_y[3],
					'crossvalidation0/y':test_y[4]
					}
			

			#This is our training loop
			process_bar=ShowProcess(iterations)
			start=time.time()
			for i in range(iterations):
				process_bar.show_process()
				if i % 500 == 0:
					print("\r the time cost is %f minute"%((time.time()-start)/60))
				#Use the data function we were passed to get a batch every iteration
				x,y,batch_index = self.data(*self.args) #execute the get_batch function each iteration(almost 20 batch in one epoch)
				#print("the batch index is ", batch_index)
				#Create feed dictionary for the batch
				feed = {
					'crossvalidation0/x':x[0],
					'crossvalidation1/x':x[1],
					'crossvalidation2/x':x[2],
					'crossvalidation3/x':x[3],
					'crossvalidation4/x':x[4],
					'crossvalidation0/y':y[0],
					'crossvalidation0/y':y[1],
					'crossvalidation0/y':y[2],
					'crossvalidation0/y':y[3],
					'crossvalidation0/y':y[4]

				}
				#Run the train op and calculate the train summary
				summary,_ = sess.run([merged,train_op],feed)
				#If we're testing, don't train on test set. But do calculate summary
				
				if test_exists:
					t_summary = sess.run(merged,test_feed)
					output_img= sess.run(self.out,test_feed)
					input_img=sess.run(self.input,test_feed)
					target_img=sess.run(self.target,test_feed)
					#Write test summary
					test_writer.add_summary(t_summary,i)
				
				#Write train summary for this step
				train_writer.add_summary(summary,i)
			process_bar.close()
			#Save our trained model		
			self.save()	
			train_writer.close() 
			test_writer.close()
		return input_img,target_img,output_img
